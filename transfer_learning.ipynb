{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqNDta-xZ2u4"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp81GSFN7uHY"
      },
      "source": [
        "* ü§Ø Tech companies and university labs have more computational resources than we do\n",
        "* üòé Let them train their super complex models on millions of images, and then re-use their kernels for our own CNNs!\n",
        "\n",
        "üéØ **<u>Goal:</u>**\n",
        "* ‚òÑÔ∏è Use a **Pretrained Neural Network** $ \\Leftrightarrow $ **Transfer learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR9-yIauZ2vA"
      },
      "source": [
        "## Google Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSm6iw9HizE"
      },
      "source": [
        "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
        "\n",
        "1. access your [Google Drive](https://drive.google.com/)\n",
        "2. go into the Colab Notebooks folder\n",
        "3. drag and drop this challenge's folder into it\n",
        "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfeVhhBZ2vC"
      },
      "source": [
        "Don't forget to enable GPU acceleration!\n",
        "\n",
        "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoRPmZtQZ2vE"
      },
      "source": [
        "When this is done, run the cells below and get to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q8uOTzh5vwJ",
        "outputId": "f3d6a56d-71a7-4799-a17a-0d2bdd303f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7HCO04lsIrcd"
      },
      "outputs": [],
      "source": [
        "# Put Colab in the context of this challenge\n",
        "import os\n",
        "\n",
        "# os.chdir allows you to change directories, like cd in the Terminal\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-transfer-learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MdAwhGJdSR"
      },
      "source": [
        "You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdd1Jv7Z2vJ"
      },
      "source": [
        "## (1) What is a Pre-Trained Neural Network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shgjzVW4Z2vJ"
      },
      "source": [
        "* Convolutions are mathematical operations designed to detect specific patterns in input images and use them to classify the images. \n",
        "* One could imagine that these patterns are not 100% specific to one task but to the input images. \n",
        "\n",
        "üöÄ **Why not re-use these kernels - whose weights have already been optimized - somewhere else?** \n",
        "- The expectation is that the trained kernels could also help us perform another classification task.\n",
        "- We are trying to ***transfer*** the knowledge of a trained CNN to a new classification task.\n",
        "\n",
        "\n",
        "üí™ Transfer Learning has two main advantages:\n",
        "- It takes less time to train a pre-trained model since we are not going to update all the weights but only some of them\n",
        "- You benefit from state-of-the-art architectures that have been trained on complex images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOkQIQj4Z2vJ"
      },
      "source": [
        "## (2) Introduction to  VGG16 \n",
        "\n",
        "üìö ***Reading Section, no code***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkNOgtqF7S_"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we will use the <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">**`VGG-16 Neural Network`**</a>.\n",
        "\n",
        "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition‚Äù. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU‚Äôs.\n",
        "\n",
        "VGG16 is a well-known architecture that has been trained on the <a href=\"https://www.image-net.org/\">**`ImageNet dataset`**</a> which is a very large database of images which belong to different categories. \n",
        "\n",
        "üëâ This architecture already learned which kernels are the best for extracting features from the images found in the `ImageNet dataset`.\n",
        "\n",
        "üëâ As you can see in the illustration, the VGG16 involves millions of parameters you don't want to retrain yourself.\n",
        "\n",
        "\n",
        "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
        "\n",
        "‚ùì How does it work in practice ‚ùì\n",
        "\n",
        "* The first layers are not specialized for the particular task the VGG16 CNN was trained on\n",
        "* Only the last dense layer is a \"classification layers\" that can be preceded with a couple of dense layers...  Therefore, we will: \n",
        "    1. Load the existing VGG16 network\n",
        "    2. Remove the last fully connected layers\n",
        "    3. Replace them with some new fully-connected layers (whose weights are randomly set)\n",
        "    4. Train these last layers on a specific classification task. \n",
        "\n",
        "üòÉ Your role is to train only the last layers for your particular problem.\n",
        "\n",
        "ü§ì We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\">**`tensorflow.keras.applications.VGG16`**</a>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-S858KRF7TA"
      },
      "source": [
        "## (3) Data loading & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzLvo4N5Z2vL"
      },
      "source": [
        "You have two options to load the data into Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6vPgeK2Z2vL"
      },
      "source": [
        "### (Option 1) Loading the data directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6FxEU-HZ2vL"
      },
      "source": [
        "* You can first get the data onto google Colab thanks to:\n",
        "\n",
        "`!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`,\n",
        "\n",
        "* and then run \n",
        "\n",
        "`!unzip flowers-dataset.zip`\n",
        "\n",
        "*This is a very easy option to load the data into your working directory.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID413NdhZ2vL"
      },
      "source": [
        "### (Option 2) Adding the data to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnMYzYgZ2vM"
      },
      "source": [
        "* You can first download the data from `https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`. \n",
        "* Then you have to add it to your Google Drive in a folder called `Deep_learning_data` (for instance)\n",
        "* And run the following code in the notebook: \n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "```\n",
        "\n",
        "* The previous code will ask you to go to a given webpage where you can copy a temporary key\n",
        "* Paste it in the cell that will appear in your Colab Notebook\n",
        "* You can now load the data on your Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrUUBluZ2vM"
      },
      "source": [
        "### Option 1 or Option 2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdS5hErJF7TB"
      },
      "source": [
        "* Why choosing option 2 over the option 1? \n",
        "    * ‚úÖ The combo Colab + Drive can be interesting if you work within a project team, and need to update the data from time to time. \n",
        "    * ‚úÖ By doing this, you can share the same data folder with your teammates, and be sure that everyone has the same dataset at any time, even though someone changes it. \n",
        "    * ‚ùå Google Colab has now access to your Google Folder..., which you may or may not be in favor of, depending on your sensibilities...\n",
        "\n",
        "---\n",
        "\n",
        "‚ùì **Question: Loading your dataset** ‚ùì \n",
        "    \n",
        "Use one of the above methods to load your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hKw_1TjOF7TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799a4938-6af3-4e6b-82e3-93696aef207b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "option_1 = False # Choose here\n",
        "\n",
        "if option_1:\n",
        "    !wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
        "    !unzip flowers-dataset.zip\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iJGBxR6HZ0Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918fb4f4-8281-44df-d2c0-ba15cf0ee871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/data-transfer-learning\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tJA_6ZUbZ0Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ab7f5d-1976-49dc-a5a7-05d697b9a124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md  transfer_learning.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onPwIzQzF7TE"
      },
      "source": [
        "‚ùì **Question:Train/Val/Test split** ‚ùì \n",
        "\n",
        "Use the following method to create \n",
        "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "CC5vyzkjZ0Ga"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_flowers_data(loading_method):\n",
        "    if loading_method == 'colab':\n",
        "        data_path = '/content/drive/My Drive/Deep_learning_data/flowers'\n",
        "    elif loading_method == 'direct':\n",
        "        data_path = 'flowers/'\n",
        "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for (cl, i) in classes.items():\n",
        "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
        "        for img in tqdm(images_path[:300]):\n",
        "            path = os.path.join(data_path, cl, img)\n",
        "            if os.path.exists(path):\n",
        "                image = Image.open(path)\n",
        "                image = image.resize((256, 256))\n",
        "                imgs.append(np.array(image))\n",
        "                labels.append(i)\n",
        "\n",
        "    X = np.array(imgs)\n",
        "    num_classes = len(set(labels))\n",
        "    y = to_categorical(labels, num_classes)\n",
        "\n",
        "    # Finally we shuffle:\n",
        "    p = np.random.permutation(len(X))\n",
        "    X, y = X[p], y[p]\n",
        "\n",
        "    first_split = int(len(imgs) /6.)\n",
        "    second_split = first_split + int(len(imgs) * 0.2)\n",
        "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
        "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Exj7vXSSZ0Gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cece83ad-3de7-488a-f8be-f096b63262c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:03<00:00, 92.90it/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:07<00:00, 41.19it/s] \n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:03<00:00, 92.56it/s] \n"
          ]
        }
      ],
      "source": [
        "# CALL load_flowers_data WITH YOUR PREFERRED METHOD HERE\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('colab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8buJbdPYfbFW",
        "outputId": "e4c234a8-5d98-4f44-de6c-2a71ac748adf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((571, 256, 256, 3), (571, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCernWQ_fiIk",
        "outputId": "378bcc27-7840-4026-9c13-076dca0271b9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm2IsMmGF7TH"
      },
      "source": [
        "‚ùì **Question: Exploring the images** ‚ùì\n",
        "\n",
        "Check the images' shapes and plot a few of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdMfP6dyF7TJ"
      },
      "source": [
        "## (4) A CNN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYcWjw1F7TJ"
      },
      "source": [
        "First, let's build our own CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wneAeLYdF7TK"
      },
      "source": [
        "‚ùì **Questions** ‚ùì \n",
        "\n",
        "1. <u>CNN Architecture and compiler:</u> Create a CNN with your own architecture and a function `load_own_model` that will be able to generate it. Some advice:\n",
        "    - Incorporate the Rescaling Layer in your Sequential architecture\n",
        "    - Add three Conv2D/MaxPooling2D combinations with an increasing number of channels and a decreasing size of kernels for example (be creative, that is not a rule of thumb, mastering CNN is an art)\n",
        "    - Don't forget the Flatten layer and some hidden layers\n",
        "    - Finish with the predictive layer\n",
        "    - Compile your CNN model accordingly\n",
        "  \n",
        "  \n",
        "2. <u>Training and comparison</u>:\n",
        "    - Train your CNN\n",
        "    - Compare its performance to a baseline accuracy\n",
        "\n",
        "<details>\n",
        "    <summary><i>Recommended architecture:</i></summary>\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
        "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "```\n",
        "\n",
        "        \n",
        "</details>        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras import layers,models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(patience = 5, restore_best_weights=True)\n",
        "model = models.Sequential()\n",
        "\n",
        "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
        "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          validation_split= 0.2,\n",
        "          epochs=100,  \n",
        "          batch_size=32, \n",
        "          verbose=1,\n",
        "          callbacks = [es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsatMKkwf7vT",
        "outputId": "5b7d08ad-a70e-42cc-b519-f1679d3082a9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 12s 84ms/step - loss: 0.9846 - accuracy: 0.4583 - val_loss: 1.3973 - val_accuracy: 0.4174\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.8881 - accuracy: 0.5746 - val_loss: 0.9106 - val_accuracy: 0.5217\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.8242 - accuracy: 0.6075 - val_loss: 0.8332 - val_accuracy: 0.5478\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.8241 - accuracy: 0.5833 - val_loss: 0.8415 - val_accuracy: 0.6000\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.7856 - accuracy: 0.6162 - val_loss: 0.8419 - val_accuracy: 0.6087\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.7616 - accuracy: 0.6557 - val_loss: 0.9600 - val_accuracy: 0.6174\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.7817 - accuracy: 0.6360 - val_loss: 0.8168 - val_accuracy: 0.5913\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 0.6706 - accuracy: 0.7237 - val_loss: 0.8166 - val_accuracy: 0.6174\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.6411 - accuracy: 0.7544 - val_loss: 0.7392 - val_accuracy: 0.6957\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 0.5991 - accuracy: 0.7588 - val_loss: 0.7986 - val_accuracy: 0.6522\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.5764 - accuracy: 0.7763 - val_loss: 0.8323 - val_accuracy: 0.6522\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.4965 - accuracy: 0.7939 - val_loss: 0.8603 - val_accuracy: 0.6609\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 1s 38ms/step - loss: 0.4349 - accuracy: 0.8202 - val_loss: 0.9753 - val_accuracy: 0.6696\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.4190 - accuracy: 0.8048 - val_loss: 1.0670 - val_accuracy: 0.6348\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f009019f970>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QpJZgrQ2Z0Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e28129-c46a-43e9-9af2-ee7e048a561e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 16ms/step - loss: 0.7178 - accuracy: 0.6913\n",
            "test_accuracy = 69.0 %\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_test,y_test)\n",
        "print(f\"test_accuracy = {round(accuracy,2)*100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DC2SFwUZ2vR"
      },
      "source": [
        "ü•° <b><u>Takeaways from building your own CNN</u></b>:\n",
        "* On an \"easy dataset\" like the MNIST, it is now easy to reach a decent accuracy. But for a more complicated problem like classifying flowers, it already becomes more challenging. Take a few minutes to play with the following link before moving on to Transfer Learning\n",
        "    * [PoloClub/CNN-Explainer](https://poloclub.github.io/cnn-explainer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8V9sny7cLP"
      },
      "source": [
        "## (5) Using a pre-trained CNN = Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxhI8bluZ2vR"
      },
      "source": [
        "As we said in the beginning, tech companies and university labs have more computational resources than we do.\n",
        "\n",
        "üî• The [**Visual Geometry Group**](https://www.robots.ox.ac.uk/~vgg/data/) *(Oxford University, Department of Science and Engineering)* became famous for some of their **Very Deep Convolutional Neural Networks**: the [**VGG16**](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
        "\n",
        "Take 7 minutes of your time to watch this incredible video of Convolutional Layers created by Dimitri Dmitriev.\n",
        "\n",
        "* üì∫ **[VGG16 Neural Network Visualization](https://www.youtube.com/watch?v=RNnKtNrsrmg)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldqCInZ2vS"
      },
      "source": [
        "### (5.1) Load VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6m5eKs9s54"
      },
      "source": [
        "‚ùì **Question: loading the VGG16** ‚ùì \n",
        "\n",
        "* Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Have a look at the documentation üìö  [tf/keras/applications/VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16)üìö\n",
        "\n",
        "* We will **load the VGG16 model** the following way:\n",
        "    - ü§Ø Let's use the **weights** learned on the [**imagenet dataset**](https://www.image-net.org/download.php) (14M pictures with 20k labels)\n",
        "    - The **`input_shape`** corresponds to the input shape of your images \n",
        "        - Note: *You have to resize them down to a consistent shape if they have different height/widths/channels*\n",
        "    - The **`include_top`** argument should be set to `False`: \n",
        "        - to avoid loading the weights of the fully-connected layers of the VGG16\n",
        "        - and also remove the last layer of the VGG16 which was specifically trained on `imagenet`\n",
        "\n",
        "<i><u>Remark:</u></i> Do not change the default value of the other arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ZemT9EuPZ0Gf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def load_model():\n",
        "  \n",
        "\n",
        "    model = VGG16(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=None,\n",
        "        input_shape=(256,256,3),\n",
        "        pooling=None,\n",
        "        classes=20000,\n",
        "        classifier_activation='softmax')\n",
        "    \n",
        "  \n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3psG3JGF7TO"
      },
      "source": [
        "‚ùì **Question: number of parameters in the VGG16** ‚ùì \n",
        "\n",
        "Look at the architecture of the model using ***.summary()***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "JtBpKLegF7TO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d60d8b-0be0-4a56-eb95-cf1806be47f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = load_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IAwxRVF7TO"
      },
      "source": [
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
        "\n",
        "üí™ Impressive, right? Two things to notice:\n",
        "- It ends with a combo Conv2D/MaxPooling2D \n",
        "- The `layers.Flatten` and the `layers.Dense` are not there yet, we need to add them.\n",
        "- There are more than 14,000,000 parameters, which is a lot... \n",
        "    - We could fine-tune them, i.e. update them as we will update the weights of the dense layers, but it will take a lot of time....\n",
        "    - For this reason, we will inform the model that the layers before the flattening will be set non-trainable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehMaUN34Z2vT"
      },
      "source": [
        "‚ùì **Question: deactivating the training of the VGG16 paramters** ‚ùì \n",
        "\n",
        "* Write a first function which:\n",
        "    - takes the previous model as the input\n",
        "    - sets the first layers to be non-trainable, by applying **`model.trainable = False`**\n",
        "    - returns the model.\n",
        "\n",
        "* Then inspect the summary of the model to check that the parameters are no longer trainable, they were set to be **`non-trainable`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "wWzKXgFnZ0Gg"
      },
      "outputs": [],
      "source": [
        "def set_nontrainable_layers(model):\n",
        "    \n",
        "    model.trainable = False\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yedT2VF7TP"
      },
      "source": [
        "‚ùì **Question: chaining the pretrained convolutional layers of VGG16 with our own dense layers** ‚ùì \n",
        "\n",
        "We will write a function that adds flattening and dense layers after the convolutional layers. To do so, we cannot directly use the classic `layers.Sequential()` instantiation.\n",
        "\n",
        "For that reason, we will discover another way here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. Have a look at this example: \n",
        "\n",
        "---\n",
        "```python\n",
        "base_model = load_model()\n",
        "base_model = set_nontrainable_layers(base_model)\n",
        "flattening_layer = layers.Flatten()\n",
        "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
        "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  flattening_layer,\n",
        "  dense_layer,\n",
        "  prediction_layer\n",
        "])\n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "* The first line loads a group of layers which is the previous VGG-16 model. \n",
        "* Then, we set these layers to be non-trainable.\n",
        "* Eventually, we can instantiate as many layers as we want.\n",
        "* Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network. \n",
        "\n",
        "Replicate the following steps by adding:\n",
        "* a flattening layer\n",
        "* two dense layers (the first with 500 neurons) to the previous VGG-16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "1joe_flQZ0Gh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def add_last_layers(model):\n",
        "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
        "    model = set_nontrainable_layers(model)\n",
        "    flattening_layer = layers.Flatten()\n",
        "    dense_layer = layers.Dense(500, activation ='relu')\n",
        "    prediction_layer = layers.Dense(3, activation='softmax')\n",
        "    \n",
        "    model = models.Sequential([\n",
        "          base_model,\n",
        "          flattening_layer,\n",
        "          dense_layer,\n",
        "          prediction_layer\n",
        "        ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-24j0psF7TQ"
      },
      "source": [
        "‚ùì **Question: inspect the parameters of a customized VGG16** ‚ùì \n",
        "\n",
        "* Now look at the layers and the parameters of your model. \n",
        "* Note that there is a distinction, at the end, between the **trainable** and **non-trainable parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "0uqGuxsJF7TR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82df2e3a-fb04-441e-98dc-d198d13c15c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_20 (Flatten)        (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 500)               16384500  \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 3)                 1503      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,100,691\n",
            "Trainable params: 16,386,003\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model = load_model()\n",
        "complete_model = add_last_layers(base_model)\n",
        "complete_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxXlnPp5F7TR"
      },
      "source": [
        "‚ùì **Question: building a function that creates a full customized VGG16 and compiles it** ‚ùì \n",
        "\n",
        "* Write a function which builds and compiles your model\n",
        "    * We advise using the _adam_ optimizer with `learning_rate=1e-4`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Q_pYYXiVZ0Gh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def build_model():\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate = 1e-4)\n",
        "\n",
        "    base_model = load_model()\n",
        "    complete_model = add_last_layers(base_model)\n",
        "    \n",
        "    complete_model.compile(loss ='categorical_crossentropy',\n",
        "                           optimizer = optimizer,\n",
        "                           metrics = ['accuracy'])\n",
        "    return complete_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96qsiKxZ2vU"
      },
      "source": [
        "### (5.2) Back to the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkwOw1eF7TS"
      },
      "source": [
        "üö® The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did _NOT_ normalize them earlier.\n",
        "\n",
        "‚ùì **Question: preprocessing the dataset** ‚ùì \n",
        "\n",
        "Apply the specific processing to the original (non-normalized) images here using the method **`preprocess_input`** that you can import from **`tensorflow.keras.applications.vgg16`**\n",
        "\n",
        "üìö Cf. [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TuYBzv1oZ0Gi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "uNeJZvtV3YDf"
      },
      "outputs": [],
      "source": [
        "X_train = preprocess_input(X_train)\n",
        "X_val = preprocess_input(X_val)\n",
        "X_test = preprocess_input(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T7HvbQfZ2vZ"
      },
      "source": [
        "### (5.3)  Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2H0KZF-EoI"
      },
      "source": [
        "\n",
        "\n",
        "‚ùì **Question: Training the customized VGG16** ‚ùì \n",
        "\n",
        "* Train the model with an Early stopping criterion on the validation accuracy -\n",
        "* Since the validation data is provided use `validation_data` instead of `validation_split`.\n",
        "\n",
        "_As usual, store the results of your training into a `history` variable._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "grmnNmjeAXcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b8de20-f0d2-4ace-f2fb-b00eb839e5f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "model = build_model()\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience = 5 , restore_best_weights= True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 32,\n",
        "                    epochs = 10000,\n",
        "                    validation_data = (X_val,y_val),\n",
        "                    callbacks = [es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kjbELY-rmWM",
        "outputId": "01e6179a-95bd-4b76-f01d-8401a51453e1"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "18/18 [==============================] - 6s 298ms/step - loss: 6.4463 - accuracy: 0.7671 - val_loss: 2.8986 - val_accuracy: 0.8212\n",
            "Epoch 2/10000\n",
            "18/18 [==============================] - 4s 248ms/step - loss: 0.6002 - accuracy: 0.9667 - val_loss: 1.9063 - val_accuracy: 0.8659\n",
            "Epoch 3/10000\n",
            "18/18 [==============================] - 4s 247ms/step - loss: 0.1664 - accuracy: 0.9877 - val_loss: 1.8840 - val_accuracy: 0.8715\n",
            "Epoch 4/10000\n",
            "18/18 [==============================] - 4s 252ms/step - loss: 0.0016 - accuracy: 0.9982 - val_loss: 1.5086 - val_accuracy: 0.8715\n",
            "Epoch 5/10000\n",
            "18/18 [==============================] - 5s 259ms/step - loss: 1.9572e-05 - accuracy: 1.0000 - val_loss: 1.4722 - val_accuracy: 0.8715\n",
            "Epoch 6/10000\n",
            "18/18 [==============================] - 4s 233ms/step - loss: 3.7733e-06 - accuracy: 1.0000 - val_loss: 1.4674 - val_accuracy: 0.8715\n",
            "Epoch 7/10000\n",
            "18/18 [==============================] - 4s 234ms/step - loss: 1.0984e-06 - accuracy: 1.0000 - val_loss: 1.4669 - val_accuracy: 0.8715\n",
            "Epoch 8/10000\n",
            "18/18 [==============================] - 4s 248ms/step - loss: 8.2071e-07 - accuracy: 1.0000 - val_loss: 1.4669 - val_accuracy: 0.8715\n",
            "Epoch 9/10000\n",
            "18/18 [==============================] - 4s 245ms/step - loss: 6.2642e-07 - accuracy: 1.0000 - val_loss: 1.4670 - val_accuracy: 0.8715\n",
            "Epoch 10/10000\n",
            "18/18 [==============================] - 4s 243ms/step - loss: 4.7344e-07 - accuracy: 1.0000 - val_loss: 1.4670 - val_accuracy: 0.8715\n",
            "Epoch 11/10000\n",
            "18/18 [==============================] - 4s 227ms/step - loss: 4.3274e-07 - accuracy: 1.0000 - val_loss: 1.4670 - val_accuracy: 0.8715\n",
            "Epoch 12/10000\n",
            "18/18 [==============================] - 4s 247ms/step - loss: 3.4924e-07 - accuracy: 1.0000 - val_loss: 1.4670 - val_accuracy: 0.8715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_I9JpiAm-W"
      },
      "source": [
        "‚ùì **Question: Looking at the accuracy** ‚ùì \n",
        "\n",
        "Plot the accuracy for both the train set and and the validation set using the usual function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "9xSw2QjtZ0Gj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
        "    if axs is not None:\n",
        "        ax1, ax2 = axs\n",
        "    else:\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
        "        exp_name = '_' + exp_name\n",
        "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
        "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
        "    #ax1.set_ylim(0., 2.2)\n",
        "    ax1.set_title('loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
        "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
        "    #ax2.set_ylim(0.25, 1.)\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.legend()\n",
        "    return (ax1, ax2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ESzinGOY6aBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "26a0526b-2e82-464d-c8f8-d92b8282e608"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<matplotlib.axes._subplots.AxesSubplot at 0x7f003809f0d0>,\n",
              " <matplotlib.axes._subplots.AxesSubplot at 0x7f00285da400>)"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3plexlQAtcC"
      },
      "source": [
        "‚ùì **Question: Evaluating the model** ‚ùì\n",
        "\n",
        "Evaluate the customized VGG16 accuracy on the test set. Did we improve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ps_9HwUyRVj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7beb1ce0-a1d1-476c-c6ae-5cabc28a8035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 3s 689ms/step - loss: 1.9780 - accuracy: 0.8456\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9780330657958984, 0.8456375598907471]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "model.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5T1KvsGZ2va"
      },
      "source": [
        "## (6) (Optional) Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF39HIb7BSOy"
      },
      "source": [
        "Now, you can try to improve the model's test accuracy. To do that, here are some options you can consider\n",
        "\n",
        "1. **Unfreeze and finetune**: Source: [Google tutorial](https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning) \n",
        ">_Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate. This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind. It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features. It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way._\n",
        "\n",
        "\n",
        "1. Add **Data Augmentation** if your model is overfitting. \n",
        "\n",
        "2. If your model is not overfitting, try a more complex model.\n",
        "\n",
        "3. Perform a precise **Grid Search** on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
        "\n",
        "4. **Change the base model** to more modern one CNN (ResNet, EfficientNet1,... available in the keras library)\n",
        "\n",
        "5. Curate the data: maintaining a sane data set is one of the keys to success.\n",
        "\n",
        "6. Collect more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3UMNBZHZ2vb"
      },
      "source": [
        "## (6.2) Comparing the performances of the CNN, the VGG, and the VGG trained on the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsfzMAQQZ0Gj"
      },
      "outputs": [],
      "source": [
        "test_accuracy_aug = res_aug[-1]\n",
        "\n",
        "\n",
        "print(f\"test_accuracy_aug = {round(test_accuracy_aug,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy_vgg = {round(test_accuracy_vgg,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")\n",
        "\n",
        "print(f'Chance level: {1./num_classes*100:.1f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gaSAxLZ2vc"
      },
      "source": [
        "---\n",
        "\n",
        "üèÅ **Congratulations** üèÅ \n",
        "\n",
        "1. Download this notebook from your `Google Drive` or directly from `Google Colab` \n",
        "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder  \n",
        "\n",
        "\n",
        "üíæ Don't forget to push your code\n",
        "\n",
        "3. Follow the usual procedure on your terminal inside the challenge folder:\n",
        "      * *git add transfer_learning.ipynb*\n",
        "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
        "      * *git push origin master*\n",
        "\n",
        "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
        "\n",
        "üöÄ If you have time, move on to the **Autoencoders** challenge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5t_7aGzZ0Gk"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}